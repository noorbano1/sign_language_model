# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12C-HCM9eML8UQYPET-B71qNEyfIsOsgh
"""

# Step 1: Install kaggle library
!pip install kaggle

# Step 2: Upload kaggle.json (API key)
from google.colab import files
files.upload()  # upload your kaggle.json file here

# Step 3: Move kaggle.json to correct folder
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Step 4: Download your dataset
!kaggle datasets download -d harshvardhan21/sign-language-detection-using-images

# Step 5: Unzip the dataset
!unzip sign-language-detection-using-images.zip -d sign_language_data

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split

import os

# Path where dataset is unzipped
dataset_path = '/content/sign_language_data/data'  # adjust if needed

# Check folders (A-Z, 0-9)
labels = os.listdir(dataset_path)
print("Labels found:", labels[:10], "...")  # just show first 10 labels

import shutil
from sklearn.model_selection import train_test_split

train_path = 'sign_language_data/train'
val_path = 'sign_language_data/val'

os.makedirs(train_path, exist_ok=True)
os.makedirs(val_path, exist_ok=True)

# Loop through each label
for label in labels:
    label_folder = os.path.join(dataset_path, label)
    images = os.listdir(label_folder)

    # Split 80% train, 20% validation
    train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)

    # Create label subfolders in train/val
    os.makedirs(os.path.join(train_path, label), exist_ok=True)
    os.makedirs(os.path.join(val_path, label), exist_ok=True)

    # Move images to train folder
    for img in train_imgs:
        src = os.path.join(label_folder, img)
        dst = os.path.join(train_path, label, img)
        shutil.copy(src, dst)

    # Move images to val folder
    for img in val_imgs:
        src = os.path.join(label_folder, img)
        dst = os.path.join(val_path, label, img)
        shutil.copy(src, dst)

print("Dataset split complete: Train & Validation folders ready!")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

train_dir = 'sign_language_data/train'
val_dir = 'sign_language_data/val'

# Training data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)

# Flow from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(64,64),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(64,64),
    batch_size=32,
    class_mode='categorical'
)

num_classes = 35  # 26 letters + 10 digits + 1 extra if any

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),  # prevent overfitting
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[early_stop, reduce_lr]
)

plt.figure(figsize=(12,4))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.legend()

plt.show()

model.save('sign_language_cnn_model.h5')

from google.colab import drive
drive.mount('/content/drive')

!cp sign_language_cnn_model.h5 /content/drive/MyDrive/

print("Number of classes:", len(labels))
print("Model output shape:", model.output_shape)

import pickle

# 35 classes as per your trained model
class_indices = {
    'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'J':9,
    'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,
    'U':20,'V':21,'W':22,'X':23,'Y':24,'0':25,'1':26,'2':27,'3':28,'4':29,
    '5':30,'6':31,'7':32,'8':33,'9':34
}

# Save labels.pkl
with open("labels.pkl", "wb") as f:
    pickle.dump(class_indices, f)

print("labels.pkl created successfully for 35 classes!")

from tensorflow.keras.preprocessing import image
from google.colab import files
import numpy as np

uploaded = files.upload()  # upload a single image

for img_name in uploaded.keys():
    img_path = img_name
    img = image.load_img(img_path, target_size=(64,64))
    img_array = image.img_to_array(img)/255.0
    img_array = np.expand_dims(img_array, axis=0)

    pred = model.predict(img_array)
    class_idx = np.argmax(pred, axis=1)[0]

    class_labels = list(train_generator.class_indices.keys())
    print("Predicted Sign:", class_labels[class_idx])